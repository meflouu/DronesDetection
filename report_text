*Background*
A background on what I did. This Tuesday I went out with @Niko R. to the test field and we took two video recordings of two types of drones, DJI phantom and another bigger one that I don't remember the name. The motivation is to calculate the (x, y) surface distance with camera in real time, not to confuse it with the depth distance (z), which the camera is not good at.

The idea is to detect the drone and segment the background out somehow and find the center point of the object detected. Then calculate the distance to the middle point of the camera stream, apply some transformation and get the distance in both axis in some unit of measurement. I choose to work with centimeters so everything you see either in the plot, the frame, the mask or the vector is all in cm unless I specify otherwise, for instance the altitude is in meters in the text frame.

*What was pre-processed*
- *The video size* was initially in 4k (3840 x 2160 x 3) at 60fps. There's absolutely no way to do real-time processing of 24M frames multiple times in 0.016s each frame and not buffer. I donwsampled and down-framed the video down to (640 x 360 x 3) at 30 fps. Which was enough to detect the drones all the time. However, there's information loss and the video gets slightly darker at each downsampling. This process can also be done execution time but it is very computationally heavy so I did it offline before running the program.
- *The interval of segmentation* is based on color channels. I made a calibration script with a little GUI to pick the right color channel intervals to keep. This was done prior to running the script, so I fed the right lower and upper bound to the script so no calibration is needed after starting it. However, in real life this will need to be done either in spot or tested beforehand.
- *The altitude* was preprocessed since we have no laser sensor data input to use. I made a function mapping to what seemed like an okay looking altitude by hardcoding altitude at given timestamps based on what we were talking about in the background audio.
- Everything else is calculated in real time and the script has no prior knowledge other than this

*What was post-processed*
- The videos were fast forwarded at approx ~3.5 times to match the audio since the recording of the screen slowed down the program. Nothing else is post-processed, all visuals are coming from script at each frame.
*Process and Reflections*
- There are six major components running in parallel for this to work: The detection of the drone, the calculation of the centroid, the calculation of the distance and the acceleration vector towards center, the validation of the mask, and the visual outputs.
- *The detection of the drone* is the only part that was different between the two videos. For the black drone the default opencv BGR channel was used because the hue and saturation of the sky background was very close to the interval of the hue and saturation of the drone object. However the blue channel was distinctively different and so the mask was based on it. The white drone (Phantom) mask was based on HSV channel. This part was so much better than I expected it to be because the program was still able to catch the drone at really low numbers of pixels when the object was almost a dot.
- *The calculation of centroid* was assumed to be the center of the rectangle detected. This is the only way possible when using color spaces. It is very effective when the drone is fully in frame but once part of the drone goes out of frame the variation is big and inaccurate.
- *The distance calculation* was just a linear estimate in the script to what seemed physically okay. Since we have not done any distance calibrations yet. Notice that the higher the altitude the more fluctuation in the distance of (x, y), because a small change results in a big shift at a high altitude.
- *The acceleration vector* is the one drawn in yellow in the frame pointing to center. It represent the force and the direction of acceleration that needs to be applied at that given moment to find the center. This will make using only one laser sensor feasible since the drone will get command to move where it can find it with the help of camera readings.
- *The validation of the mask* is a very important step to know whether you're actually detecting the object of interest or just a random object. That's why it's good to have on the side throughout all frames to log false detections. In the script the interval boundary was separated enough that this didn't happen, but in real life it can occur.
- *The visuals* include the frame that has altitude and frame per second information as well as a dot in a different color representing where the algorithm think the center of drone is. There are two real time plots both in 3d and 2d. The 3d plot has a sliding z axis of 5 meters because making it static made the graph look so tiny.
- *Optimisation* of the algorithm took by far the most amount of work in this project. When I started it was around 3 seconds per frames (not to confuse it with 3 frames per second with is 9 times faster). Now it's working at around 23fps without plotting, and ~12fps with plotting when not screen recording.  The real-time plotting was especially expensive and it drains all the CPU usage but there were low level techniques to tackle this. An overview of what part took the most execution time can be found here https://i.imgur.com/DzlXF8D.png
https://i.imgur.com/DzlXF8D.png
 (edited)
*What will be a challenge*
- Developing a segmentation mask that can work for general situation is really difficult. With Colors only it's not possible. The videos were taking a few minutes apart from each other, however the color spaces of the background was very different (to my surprise). This is due to many factors including the brightness adaptation of the camera and the change of exposure of moving objects. However, this can be controlled with professional cameras and there are more advanced techniques to do object detection such as building a NN model or more traditionally using Homography and feature matching once the drone to use is specified.
- I showed my algorithm to some image processing researchers in TUT to get their feedback and one thing I didn't think about was camera calibration. To calculate the distance accurately you must calibrate the camera beforehand (only once) and get the exact focal length and the optical centre, which is not the same as the centre-most pixel due to manufacturing errors. There is also radial distortions that needs to be taken care of, usually using a set of checkerboard images with binary-colored squares of similar distances. I am not sure how this works yet but it's something to consider.
- The distance is not linear, a change of double pixels does not translate to double distance. The pixel distance sometimes varies even between similar cameras of the same vendors, but calibration step should take care of this.
- Calculating the exact center of the drone is a challenge. The drone segmentation does not always detect all parts equally so the average centre is not always the correct one. One way to go around this is to put a distinct small object like a colored paper in the drone centre and detect it. In the altitude we are currently dealing with, very high resolution would be required.
- The hardware implementation is obviously a challenge in this case due to the high level of precision needed. A perfect system would be one that is fully calibrated, however, an error of inclination can also be modeled mathematically but this will increase the complexity, and most importantly, lower the trust in the validation of the system.

*What will not be a challenge*
- The distance accuracy can be as accurate as the number of pixels. With the use of professional camera that can zoom and get a window of a small distance with high resolution the accuracy can be much more accurate than current GPS.
- The computations for this algorithm will not be that heavy in a real system because no real-time plots will be needed but also because the Region of interest can be limited and the algorithm window will be smaller since in the main task the drone will be hovering still, rather than moving in large distances like what we did just to test.